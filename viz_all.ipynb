{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf406e4-16c3-4c21-8845-01ff9f016612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mykernel/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading files: 100%|██████████| 15/15 [00:30<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils import download_all_models, suppress_stdout_stderr\n",
    "\n",
    "TOKEN = \"\"\n",
    "download_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f88a220-5390-446a-9030-b91239d9b638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/soyatrans_nirmal.pth',\n",
       " 'models/soyatrans_pungliya.pth',\n",
       " 'models/soyatrans_mendeley.pth',\n",
       " 'models/maianet_nirmal.pth',\n",
       " 'models/maianet_pungliya.pth',\n",
       " 'models/maianet_mendeley.pth',\n",
       " 'models/tswinf_nirmal.pth',\n",
       " 'models/tswinf_pungliya.pth',\n",
       " 'models/tswinf_mendeley.pth',\n",
       " 'models/convnext_nirmal.pth',\n",
       " 'models/convnext_pungliya.pth',\n",
       " 'models/convnext_mendeley.pth',\n",
       " 'models/coreplant_nirmal.pth',\n",
       " 'models/coreplant_pungliya.pth',\n",
       " 'models/coreplant_mendeley.pth']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maianet import MaiaNet\n",
    "from soyatrans import SoyaTrans\n",
    "from build import build_model\n",
    "from convnext import Autoencoder\n",
    "from coreplant import Classifier\n",
    "\n",
    "model_objects = [SoyaTrans, MaiaNet, build_model, Autoencoder, Classifier]\n",
    "model_names = [\"soyatrans\",\"maianet\", \"tswinf\", \"convnext\", \"coreplant\"]\n",
    "\n",
    "datasets = [\"nirmal\", \"pungliya\", \"mendeley\"]\n",
    "dataset_num_classes = [5, 3, 11]\n",
    "\n",
    "model_files = [f\"models/{model}_{dataset}.pth\" for model in model_names for dataset in datasets]\n",
    "model_instances = [model(num_classes = num_classes) for model in model_objects for num_classes in dataset_num_classes]\n",
    "\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb14c4de-d82a-4739-9257-fa04222d24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "vdata = datasets.ImageFolder(\"vizdata/Visualization/pungliyavithika\", transform=transform)\n",
    "ndata = datasets.ImageFolder(\"vizdata/Visualization/nirmalsankana\", transform=transform)\n",
    "mdata = datasets.ImageFolder(\"vizdata/Visualization/mendeley\", transform=transform)\n",
    "\n",
    "def match_dataset(model_name):\n",
    "    if \"nirmal\" in model_name:\n",
    "        dataset = ndata\n",
    "        dataset_name = 'nirmal'\n",
    "    elif \"mendeley\" in model_name:\n",
    "        dataset = mdata\n",
    "        dataset_name = 'mendeley'\n",
    "    else:\n",
    "        dataset = vdata\n",
    "        dataset_name = 'pungliya'\n",
    "    return dataset, dataset_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe1d8e5-4627-48a2-a104-533c0bed9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/soyatrans_nirmal.pth loaded\n",
      "models/soyatrans_pungliya.pth loaded\n",
      "models/soyatrans_mendeley.pth loaded\n",
      "models/maianet_nirmal.pth loaded\n",
      "models/maianet_pungliya.pth loaded\n",
      "models/maianet_mendeley.pth loaded\n",
      "models/tswinf_nirmal.pth loaded\n",
      "models/tswinf_pungliya.pth loaded\n",
      "models/tswinf_mendeley.pth loaded\n",
      "models/convnext_nirmal.pth loaded\n",
      "models/convnext_pungliya.pth loaded\n",
      "models/convnext_mendeley.pth loaded\n",
      "models/coreplant_nirmal.pth loaded\n",
      "models/coreplant_pungliya.pth loaded\n",
      "models/coreplant_mendeley.pth loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TSwinModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out[\"sup\"]  # only return the classification output since it is a Munch object\n",
    "\n",
    "def load_instance(model, file):\n",
    "    checkpoint = torch.load(f\"{file}\")\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(f\"{file} loaded\")\n",
    "\n",
    "    if \"maianet\" in file:\n",
    "        target_layer = [model.maia_4.conv3[0]]\n",
    "    elif \"soyatrans\" in file:\n",
    "        target_layer = [model.stage1.downsample]\n",
    "    elif \"tswinf\" in file:\n",
    "        target_layer = [model.stage4[-1].attns[0].get_v]\n",
    "        model = TSwinModel(model)\n",
    "        # target_layer = [model.LCA.conv1[0]]\n",
    "    elif 'coreplant' in file:\n",
    "        target_layer = [model.encoder.model.conv_head]\n",
    "    elif 'convnext' in file:\n",
    "        target_layer = [model.encoder.stage4[1]]\n",
    "    return model, target_layer\n",
    "\n",
    "models = {file: load_instance(model, file) for file, model in zip(model_files, model_instances)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf787cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import (\n",
    "    GradCAM,\n",
    "    GradCAMPlusPlus,\n",
    "    ScoreCAM,  # Needed for isinstance check\n",
    ")\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def tensor_to_rgb_image(tensor):\n",
    "    img = tensor.clone().detach().cpu()\n",
    "    img = img * 0.5 + 0.5  # reverse normalization\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "    \n",
    "def plot(model, image, image_path, class_index, cams, output_dir=\"output\", model_name=\"default_model\"):\n",
    "    results = {}\n",
    "    paths = {}\n",
    "\n",
    "    # Get class folder and image base name\n",
    "    class_folder = os.path.basename(os.path.dirname(image_path))  # e.g., 'class1'\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]  # e.g., 'image1'\n",
    "\n",
    "    # Final output path: output/model_name/class_folder/\n",
    "    final_output_dir = os.path.join(output_dir, model_name, class_folder)\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = image.unsqueeze(0).to(device)\n",
    "    rgb_img = tensor_to_rgb_image(image)\n",
    "    target = [ClassifierOutputTarget(class_index)]\n",
    "\n",
    "    # Save original image\n",
    "    original_path = os.path.join(final_output_dir, f\"{base_name}_original.jpg\")\n",
    "    original_image = Image.fromarray((rgb_img * 255).astype(np.uint8))\n",
    "    original_image.save(original_path)\n",
    "    results['original'] = original_image\n",
    "    paths['original'] = original_path\n",
    "\n",
    "    # Save CAM images\n",
    "    for name, cam_method in cams.items():\n",
    "        if isinstance(cam_method, ScoreCAM):\n",
    "            with torch.no_grad():\n",
    "                grayscale_cam = cam_method(input_tensor=input_tensor, targets=target)[0]\n",
    "        else:\n",
    "            grayscale_cam = cam_method(input_tensor=input_tensor, targets=target)[0]\n",
    "\n",
    "        cam_image = Image.fromarray(show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True))\n",
    "        save_path = os.path.join(final_output_dir, f\"{base_name}_{name}.jpg\")\n",
    "        cam_image.save(save_path)\n",
    "        results[name] = cam_image\n",
    "        paths[name] = save_path\n",
    "\n",
    "    return results, paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f64b663-26b6-4120-9813-17c792a06b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_models = []\n",
    "done_models.append(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591644f5-0827-4382-98a9-dfaae65a240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soyatrans_pungliya.pth pungliya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "soyatrans_pungliya.pth: 100%|██████████| 45/45 [01:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soyatrans_mendeley.pth mendeley\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "soyatrans_mendeley.pth: 100%|██████████| 165/165 [05:34<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maianet_nirmal.pth nirmal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "maianet_nirmal.pth: 100%|██████████| 75/75 [04:07<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maianet_pungliya.pth pungliya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "maianet_pungliya.pth: 100%|██████████| 45/45 [02:27<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maianet_mendeley.pth mendeley\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "maianet_mendeley.pth: 100%|██████████| 165/165 [09:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tswinf_nirmal.pth nirmal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tswinf_nirmal.pth: 100%|██████████| 75/75 [03:21<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tswinf_pungliya.pth pungliya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tswinf_pungliya.pth: 100%|██████████| 45/45 [01:59<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tswinf_mendeley.pth mendeley\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tswinf_mendeley.pth: 100%|██████████| 165/165 [07:21<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_nirmal.pth nirmal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convnext_nirmal.pth: 100%|██████████| 75/75 [00:28<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_pungliya.pth pungliya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convnext_pungliya.pth: 100%|██████████| 45/45 [00:16<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_mendeley.pth mendeley\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convnext_mendeley.pth: 100%|██████████| 165/165 [01:02<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coreplant_nirmal.pth nirmal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coreplant_nirmal.pth: 100%|██████████| 75/75 [02:11<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coreplant_pungliya.pth pungliya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coreplant_pungliya.pth: 100%|██████████| 45/45 [01:18<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coreplant_mendeley.pth mendeley\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coreplant_mendeley.pth: 100%|██████████| 165/165 [04:47<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for model_name, model_item in models.items():\n",
    "\n",
    "    model_name = model_name.removeprefix('models/')\n",
    "\n",
    "    if model_name in done_models:\n",
    "        continue\n",
    "    model, target_layers = model_item\n",
    "    dataset, dataset_name = match_dataset(model_name)\n",
    "    class_names = dataset.classes\n",
    "    print(model_name, dataset_name)\n",
    "\n",
    "    model = model.cuda() if torch.cuda.is_available() else model.cpu()\n",
    "    model.eval()\n",
    "\n",
    "    cams = {\n",
    "        \"Grad-CAM\": GradCAM(model=model, target_layers=target_layers),\n",
    "        \"Grad-CAM++\": GradCAMPlusPlus(model=model, target_layers=target_layers),\n",
    "        \"Score-CAM\": ScoreCAM(model=model, target_layers=target_layers),\n",
    "    }\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=f\"{model_name}\", leave=True) as pbar:\n",
    "        for idx in range(len(dataset)):\n",
    "            image, label = dataset[idx]\n",
    "            image_path, _ = dataset.imgs[idx]\n",
    "\n",
    "            # Suppress plot outputs\n",
    "            with suppress_stdout_stderr():\n",
    "            # if True:\n",
    "                input_tensor = image.unsqueeze(0).to(device)\n",
    "                output_dir = f\"output/{model_name}\"\n",
    "\n",
    "                cam_results, paths = plot(model, image, image_path, label, cams, output_dir=\"output\", model_name =model_name)\n",
    "                    \n",
    "                data_entry = {\n",
    "                    \"model\": model_name,\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"original\": cam_results['original'],\n",
    "                    \"gradcam\": cam_results[\"Grad-CAM\"],\n",
    "                    \"gradcam++\": cam_results[\"Grad-CAM++\"],\n",
    "                    \"scorecam\": cam_results[\"Score-CAM\"],\n",
    "                    \"label\": class_names[label],\n",
    "                    \"label_index\": label,\n",
    "                    \"paths\":paths\n",
    "                }\n",
    "                all_data.append(data_entry)\n",
    "            pbar.update(1)\n",
    "        \n",
    "    del model, cams\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    done_models.append(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af4af2e-4824-49a9-9ccc-d41a628028e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A:   0%|          | 0/1350 [00:00<?, ? examples/s]\n",
      "\u001b[A:  15%|█▍        | 200/1350 [00:00<00:00, 1513.42 examples/s]\n",
      "\u001b[A:  37%|███▋      | 500/1350 [00:00<00:00, 1204.16 examples/s]\n",
      "\u001b[A:  67%|██████▋   | 900/1350 [00:00<00:00, 1769.54 examples/s]\n",
      "Map: 100%|██████████| 1350/1350 [00:00<00:00, 1481.23 examples/s]\n",
      "\n",
      "\u001b[Aating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]\n",
      "\u001b[Aating parquet from Arrow format:  29%|██▊       | 4/14 [00:00<00:00, 31.04ba/s]\n",
      "\u001b[Aating parquet from Arrow format:  57%|█████▋    | 8/14 [00:00<00:00, 27.71ba/s]\n",
      "\u001b[Aating parquet from Arrow format:  79%|███████▊  | 11/14 [00:00<00:00, 25.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 14/14 [00:00<00:00, 25.50ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/omkar334/agri_viz/commit/64ebb277f715e776f0d17d6acb8700330287f08e', commit_message='Upload dataset', commit_description='', oid='64ebb277f715e776f0d17d6acb8700330287f08e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/omkar334/agri_viz', endpoint='https://huggingface.co', repo_type='dataset', repo_id='omkar334/agri_viz'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from utils import upload\n",
    "# models = [f\"models/{i}\" for i in os.listdir('models') if i.endswith('.pth')]\n",
    "# upload(models)\n",
    "\n",
    "from datasets import Dataset, Features, Image as HFImage\n",
    "\n",
    "columns = {key: [d[key] for d in all_data] for key in all_data[0]}\n",
    "\n",
    "upload_dataset = Dataset.from_dict(columns)\n",
    "\n",
    "# Convert columns to Image features after dataset creation\n",
    "upload_dataset = upload_dataset.cast_column(\"original\", HFImage())\n",
    "upload_dataset = upload_dataset.cast_column(\"gradcam\", HFImage())\n",
    "upload_dataset = upload_dataset.cast_column(\"gradcam++\", HFImage())\n",
    "upload_dataset = upload_dataset.cast_column(\"scorecam\", HFImage())\n",
    "\n",
    "upload_dataset.push_to_hub(\"agri_viz\", token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da52d25-dfee-481d-bbdb-e15e271f1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r output.zip output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1315fd1-3813-4fe4-8770-fb2bdd361f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading output.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aput.zip:   0%|          | 0.00/57.4M [00:00<?, ?B/s]\n",
      "\u001b[Aput.zip:   1%|▏         | 803k/57.4M [00:00<00:07, 7.98MB/s]\n",
      "\u001b[Aput.zip:   4%|▍         | 2.54M/57.4M [00:00<00:04, 13.3MB/s]\n",
      "\u001b[Aput.zip:  28%|██▊       | 16.0M/57.4M [00:00<00:01, 28.8MB/s]\n",
      "\u001b[Aput.zip:  38%|███▊      | 21.7M/57.4M [00:00<00:01, 31.9MB/s]\n",
      "\u001b[Aput.zip:  43%|████▎     | 24.7M/57.4M [00:00<00:01, 29.6MB/s]\n",
      "\u001b[Aput.zip:  56%|█████▌    | 32.0M/57.4M [00:01<00:00, 25.6MB/s]\n",
      "output.zip: 100%|██████████| 57.4M/57.4M [00:01<00:00, 33.0MB/s]\n",
      "Uploading files: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import upload\n",
    "\n",
    "paths = ['output.zip']\n",
    "upload(paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mykernel)",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
